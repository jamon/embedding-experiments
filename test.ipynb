{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3bbe208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: My favorite woman in history is Marie Curie\n",
      "Query: I think she's most known for being a pharoah, but I'm not sure.  What is she known for?\n",
      "Passages:\n",
      "  1. Marie Antoinette is most known for being the last Queen of France before the French Revolution and her alleged quote 'Let them eat cake'.\n",
      "  2. Cleopatra is most known for being the last pharaoh of Ancient Egypt and her relationships with Julius Caesar and Mark Antony.\n",
      "  3. Joan of Arc is most known for leading the French army against English occupation during the Hundred Years' War.\n",
      "  4. Rosa Parks is most known for refusing to give up her bus seat to a white passenger, sparking the Montgomery Bus Boycott.\n",
      "  5. Marie Curie is most known for being the first woman to win a Nobel Prize and discovering the elements polonium and radium.\n",
      "  6. Frida Kahlo is most known for her self-portraits and paintings that explored identity, postcolonialism, and the female experience.\n",
      "  7. Virginia Woolf is most known for her modernist novels like Mrs. Dalloway and To the Lighthouse.\n",
      "  8. Eleanor Roosevelt is most known for her role in drafting the Universal Declaration of Human Rights and her advocacy for civil rights.\n",
      "  9. Amelia Earhart is most known for being the first female aviator to fly solo across the Atlantic Ocean.\n",
      "\n",
      "\n",
      "### Loading EmbeddingGemma model ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer 0: Transformer\n",
      "  Layer 1: Pooling\n",
      "  Layer 2: Dense\n",
      "  Layer 3: Dense\n",
      "  Layer 4: Normalize\n",
      "\n",
      "Loaded model with tokenizer: GemmaTokenizerFast\n",
      "Base transformer: Gemma3TextModel\n",
      "Found 2 Dense projection layers\n",
      "\n",
      "\n",
      "### Computing passage embeddings (once) ###\n",
      "Computed 9 passage embeddings with shape torch.Size([9, 768])\n",
      "\n",
      "\n",
      "### APPROACH 1: Context in attention only (custom masked pooling) ###\n",
      "This is the CORE approach: context influences via attention, but we pool only over query tokens\n",
      "\n",
      "[Approach 1 - Query] Pooling Operation:\n",
      "  Input shape: torch.Size([1, 46, 768])\n",
      "  Pooling mask shape: torch.Size([1, 46])\n",
      "  Batch 0: pooling over 35/46 tokens\n",
      "    (sequence padded from 35 to 46)\n",
      "  Output shape: torch.Size([1, 768])\n",
      "\n",
      "Approach 1 - Similarity scores (top 3):\n",
      "  Query vs Passage 2: 46.68\n",
      "  Query vs Passage 5: 43.53\n",
      "  Query vs Passage 8: 31.41\n",
      "\n",
      "\n",
      "### APPROACH 2: No context (baseline) ###\n",
      "\n",
      "[Approach 2 - Query] Pooling Operation:\n",
      "  Input shape: torch.Size([1, 37, 768])\n",
      "  Pooling mask shape: torch.Size([1, 37])\n",
      "  Batch 0: pooling over 37/37 tokens\n",
      "  Output shape: torch.Size([1, 768])\n",
      "\n",
      "Approach 2 - Similarity scores (top 3):\n",
      "  Query vs Passage 2: 54.48\n",
      "  Query vs Passage 1: 35.77\n",
      "  Query vs Passage 9: 32.54\n",
      "\n",
      "\n",
      "### APPROACH 3: Context included in the embedding ###\n",
      "\n",
      "[Approach 3 - Query] Pooling Operation:\n",
      "  Input shape: torch.Size([1, 45, 768])\n",
      "  Pooling mask shape: torch.Size([1, 45])\n",
      "  Batch 0: pooling over 45/45 tokens\n",
      "  Output shape: torch.Size([1, 768])\n",
      "\n",
      "Approach 3 - Similarity scores (top 3):\n",
      "  Query vs Passage 5: 55.66\n",
      "  Query vs Passage 2: 46.19\n",
      "  Query vs Passage 1: 37.19\n",
      "\n",
      "\n",
      "### APPROACH 4: Standard EmbeddingGemma encode_query method ###\n",
      "\n",
      "Approach 4 - Similarity scores (top 3):\n",
      "  Query vs Passage 2: 54.48\n",
      "  Query vs Passage 1: 35.77\n",
      "  Query vs Passage 9: 32.54\n",
      "\n",
      "Verification: Approach 4 vs Approach 2\n",
      "  Max difference in scores: 0.000011\n",
      "  Embeddings identical: True\n",
      "\n",
      "\n",
      "============================================================\n",
      "SUMMARY - EmbeddingGemma with Custom Masked Pooling\n",
      "============================================================\n",
      "\n",
      "Approach 1 (Context in attention only - CORE EXPERIMENT):\n",
      "  ✓ Query embedding represents ONLY the query text\n",
      "  ✓ Context influences interpretation via attention\n",
      "  ✓ Query embedding remains comparable to passages\n",
      "  Top match: Passage 2 (score: 46.68)\n",
      "\n",
      "Approach 2 (No context):\n",
      "  ✗ No way to use context to influence query interpretation\n",
      "  Top match: Passage 2 (score: 54.48)\n",
      "\n",
      "Approach 3 (Context in embedding):\n",
      "  ✓ Can use context to influence query\n",
      "  ✗ Query embedding now represents 'context + query', not just query\n",
      "  ✗ Changes the semantic representation of the query itself\n",
      "  Top match: Passage 5 (score: 55.66)\n",
      "\n",
      "Approach 4 (Standard EmbeddingGemma with prompts):\n",
      "  • Uses built-in encode_query with task-specific prompts\n",
      "  • Different from Approach 2 due to prompt engineering\n",
      "  Top match: Passage 2 (score: 54.48)\n",
      "\n",
      "============================================================\n",
      "KEY INSIGHT: Approach 1 demonstrates masked pooling where\n",
      "context is used during attention (influencing hidden states)\n",
      "but excluded from the final embedding via selective pooling.\n",
      "This allows context to guide interpretation without changing\n",
      "the semantic space of the query embedding.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 1. Helper: masked average pooling\n",
    "#########################################\n",
    "\n",
    "def masked_average_pool(last_hidden_states: Tensor,\n",
    "                        pooling_mask: Tensor,\n",
    "                        label: str = \"\") -> Tensor:\n",
    "    \"\"\"\n",
    "    last_hidden_states: [batch, seq_len, hidden_dim]\n",
    "    pooling_mask:       [batch, seq_len] with 1 for tokens to include, 0 otherwise\n",
    "    label:              optional label for logging\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, hidden_dim = last_hidden_states.shape\n",
    "    \n",
    "    if label:\n",
    "        print(f\"\\n[{label}] Pooling Operation:\")\n",
    "        print(f\"  Input shape: {last_hidden_states.shape}\")\n",
    "        print(f\"  Pooling mask shape: {pooling_mask.shape}\")\n",
    "    \n",
    "    # Expand mask to [batch, seq_len, 1]\n",
    "    mask = pooling_mask[..., None].bool()\n",
    "\n",
    "    # Zero-out tokens we don't want to pool over\n",
    "    masked_hidden = last_hidden_states.masked_fill(~mask, 0.0)\n",
    "\n",
    "    # Sum over tokens\n",
    "    summed = masked_hidden.sum(dim=1)\n",
    "\n",
    "    # Count how many tokens are actually included per example\n",
    "    counts = pooling_mask.sum(dim=1)[..., None].clamp(min=1)\n",
    "    \n",
    "    if label:\n",
    "        for i in range(min(batch_size, 3)):  # Show first 3 examples\n",
    "            num_tokens = int(counts[i, 0].item())\n",
    "            total_seq_len = (pooling_mask[i] != 0).sum().item()\n",
    "            print(f\"  Batch {i}: pooling over {num_tokens}/{seq_len} tokens\")\n",
    "            if total_seq_len < seq_len:\n",
    "                print(f\"    (sequence padded from {total_seq_len} to {seq_len})\")\n",
    "\n",
    "    # Return average\n",
    "    result = summed / counts\n",
    "    \n",
    "    if label:\n",
    "        print(f\"  Output shape: {result.shape}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 2. Build inputs with explicit context for custom approach\n",
    "#########################################\n",
    "\n",
    "def build_inputs_with_context_custom(contexts, texts, tokenizer):\n",
    "    \"\"\"\n",
    "    Build custom inputs where context is in attention but not in pooling.\n",
    "    \n",
    "    contexts: list[str]  -- extra context for each example\n",
    "    texts:    list[str]  -- main text we want to embed\n",
    "\n",
    "    Returns:\n",
    "      batch_dict      -- dict with input_ids, attention_mask for the model\n",
    "      pooling_mask    -- tensor [batch, seq_len], 1 only on \"text\" tokens\n",
    "    \"\"\"\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    pooling_masks = []\n",
    "\n",
    "    for ctx, txt in zip(contexts, texts):\n",
    "        # Tokenize WITHOUT special tokens so we can control layout\n",
    "        ctx_ids = tokenizer.encode(ctx, add_special_tokens=False)\n",
    "        txt_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "\n",
    "        # Layout: [BOS] context [EOS] text [EOS]\n",
    "        ids = [bos_id] + ctx_ids + [eos_id] + txt_ids + [eos_id]\n",
    "\n",
    "        # Model attention mask: attend to everything except padding\n",
    "        attn = [1] * len(ids)\n",
    "\n",
    "        # Pooling mask:\n",
    "        #   0 for [BOS], context, and first [EOS]\n",
    "        #   1 for text tokens\n",
    "        #   0 for final [EOS]\n",
    "        num_ctx = len(ctx_ids)\n",
    "        num_txt = len(txt_ids)\n",
    "\n",
    "        pool = (\n",
    "            [0] * (1 + num_ctx + 1) +  # [BOS] + context + [EOS]\n",
    "            [1] * num_txt +            # text tokens\n",
    "            [0]                        # final [EOS]\n",
    "        )\n",
    "\n",
    "        input_ids.append(ids)\n",
    "        attention_masks.append(attn)\n",
    "        pooling_masks.append(pool)\n",
    "\n",
    "    # Pad sequences to same length for batching\n",
    "    batch = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids, \"attention_mask\": attention_masks},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Pad the pooling masks with 0\n",
    "    pooling_masks = tokenizer.pad(\n",
    "        {\"input_ids\": pooling_masks},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    return batch, pooling_masks\n",
    "\n",
    "\n",
    "def apply_dense_layers(embeddings: Tensor, dense_layers: list) -> Tensor:\n",
    "    \"\"\"\n",
    "    Apply Dense projection layers to embeddings.\n",
    "    EmbeddingGemma has 2 Dense layers after pooling.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        features = {\"sentence_embedding\": embeddings}\n",
    "        for dense in dense_layers:\n",
    "            features = dense(features)\n",
    "        # Clone to avoid inference mode issues\n",
    "        return features[\"sentence_embedding\"].clone()\n",
    "\n",
    "\n",
    "def compute_and_print_scores(query_embedding: Tensor, \n",
    "                              passage_embeddings: Tensor,\n",
    "                              approach_name: str) -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute similarity scores and print top 3 results.\n",
    "    Returns the scores tensor and top indices.\n",
    "    \"\"\"\n",
    "    # Ensure query_embedding is 2D\n",
    "    if query_embedding.dim() == 1:\n",
    "        query_embedding = query_embedding.unsqueeze(0)\n",
    "    \n",
    "    scores = (query_embedding @ passage_embeddings.T) * 100\n",
    "    print(f\"\\n{approach_name} - Similarity scores (top 3):\")\n",
    "    top_indices = scores[0].argsort(descending=True)[:3]\n",
    "    for idx in top_indices:\n",
    "        print(f\"  Query vs Passage {idx.item() + 1}: {scores[0, idx].item():.2f}\")\n",
    "    return scores, top_indices\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 3. Example data - CONTRIVED SCENARIO\n",
    "#########################################\n",
    "\n",
    "query = \"I think she's most known for being a pharoah, but I'm not sure.  What is she known for?\"\n",
    "\n",
    "passages = [\n",
    "    \"Marie Antoinette is most known for being the last Queen of France before the French Revolution and her alleged quote 'Let them eat cake'.\",\n",
    "    \"Cleopatra is most known for being the last pharaoh of Ancient Egypt and her relationships with Julius Caesar and Mark Antony.\",\n",
    "    \"Joan of Arc is most known for leading the French army against English occupation during the Hundred Years' War.\",\n",
    "    \"Rosa Parks is most known for refusing to give up her bus seat to a white passenger, sparking the Montgomery Bus Boycott.\",\n",
    "    \"Marie Curie is most known for being the first woman to win a Nobel Prize and discovering the elements polonium and radium.\",\n",
    "    \"Frida Kahlo is most known for her self-portraits and paintings that explored identity, postcolonialism, and the female experience.\",\n",
    "    \"Virginia Woolf is most known for her modernist novels like Mrs. Dalloway and To the Lighthouse.\",\n",
    "    \"Eleanor Roosevelt is most known for her role in drafting the Universal Declaration of Human Rights and her advocacy for civil rights.\",\n",
    "    \"Amelia Earhart is most known for being the first female aviator to fly solo across the Atlantic Ocean.\",\n",
    "]\n",
    "\n",
    "context = \"My favorite woman in history is Marie Curie\"\n",
    "\n",
    "# Print the data we'll be working with\n",
    "print(\"Context:\", context)\n",
    "print(\"Query:\", query)\n",
    "print(\"Passages:\")\n",
    "for i, passage in enumerate(passages):\n",
    "    print(f\"  {i+1}. {passage}\")\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 4. Load model\n",
    "#########################################\n",
    "\n",
    "print(\"\\n\\n### Loading EmbeddingGemma model ###\")\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "\n",
    "# Get the underlying components for custom approaches\n",
    "tokenizer = None\n",
    "base_transformer = None\n",
    "dense_layers = []\n",
    "\n",
    "for i, module in enumerate(model):\n",
    "    print(f\"  Layer {i}: {type(module).__name__}\")\n",
    "    if hasattr(module, 'tokenizer'):\n",
    "        tokenizer = module.tokenizer\n",
    "    if hasattr(module, 'auto_model'):\n",
    "        base_transformer = module.auto_model\n",
    "    # Collect Dense layers (there are 2 in EmbeddingGemma)\n",
    "    if type(module).__name__ == 'Dense':\n",
    "        dense_layers.append(module)\n",
    "\n",
    "if tokenizer is None:\n",
    "    raise ValueError(\"Could not find tokenizer in model modules\")\n",
    "if base_transformer is None:\n",
    "    raise ValueError(\"Could not find base transformer in model modules\")\n",
    "\n",
    "print(f\"\\nLoaded model with tokenizer: {type(tokenizer).__name__}\")\n",
    "print(f\"Base transformer: {type(base_transformer).__name__}\")\n",
    "print(f\"Found {len(dense_layers)} Dense projection layers\")\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 4.5. Compute passage embeddings ONCE (used by all approaches)\n",
    "#########################################\n",
    "print(\"\\n\\n### Computing passage embeddings (once) ###\")\n",
    "\n",
    "# Use the standard encode_document method - already normalized\n",
    "passage_embeddings = model.encode_document(passages, convert_to_tensor=True)\n",
    "print(f\"Computed {len(passages)} passage embeddings with shape {passage_embeddings.shape}\")\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 5. APPROACH 1: Context in attention, NOT in pooling (CORE EXPERIMENT)\n",
    "#########################################\n",
    "print(\"\\n\\n### APPROACH 1: Context in attention only (custom masked pooling) ###\")\n",
    "print(\"This is the CORE approach: context influences via attention, but we pool only over query tokens\")\n",
    "\n",
    "# Get device\n",
    "device = base_transformer.device\n",
    "\n",
    "# Add the standard EmbeddingGemma query prompt to the query text only\n",
    "# The context remains unprompted\n",
    "query_prompt = \"task: search result | query: \"\n",
    "query_with_prompt = f\"{query_prompt}{query}\"\n",
    "\n",
    "# Build custom inputs with context in attention but not in pooling\n",
    "batch, pooling_mask = build_inputs_with_context_custom(\n",
    "    contexts=[context],\n",
    "    texts=[query_with_prompt],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "pooling_mask = pooling_mask.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Use the base transformer directly\n",
    "    outputs = base_transformer(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "    )\n",
    "\n",
    "last_hidden = outputs.last_hidden_state\n",
    "\n",
    "# Pool ONLY over the \"main text\" tokens (exclude context)\n",
    "query_embedding = masked_average_pool(last_hidden, pooling_mask, label=\"Approach 1 - Query\")\n",
    "\n",
    "# Apply Dense projection layers (same as standard pipeline)\n",
    "query_embedding = apply_dense_layers(query_embedding, dense_layers)\n",
    "\n",
    "# Normalize\n",
    "query_embedding = F.normalize(query_embedding, p=2, dim=1)\n",
    "\n",
    "# Calculate and print scores\n",
    "scores, top_indices = compute_and_print_scores(query_embedding, passage_embeddings, \"Approach 1\")\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 6. APPROACH 2: NO context at all (baseline)\n",
    "#########################################\n",
    "print(\"\\n\\n### APPROACH 2: No context (baseline) ###\")\n",
    "\n",
    "# Add the standard EmbeddingGemma query prompt\n",
    "query_prompt = \"task: search result | query: \"\n",
    "query_with_prompt = f\"{query_prompt}{query}\"\n",
    "\n",
    "# Just embed the query without any context (but with prompt)\n",
    "query_no_context_batch = tokenizer([query_with_prompt], padding=True, return_tensors=\"pt\")\n",
    "query_no_context_batch = {k: v.to(device) for k, v in query_no_context_batch.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_no_context_outputs = base_transformer(**query_no_context_batch)\n",
    "\n",
    "# Standard average pooling over all non-padding tokens\n",
    "query_embedding_nc = masked_average_pool(\n",
    "    query_no_context_outputs.last_hidden_state,\n",
    "    query_no_context_batch[\"attention_mask\"],\n",
    "    label=\"Approach 2 - Query\"\n",
    ")\n",
    "\n",
    "# Apply Dense projection layers\n",
    "query_embedding_nc = apply_dense_layers(query_embedding_nc, dense_layers)\n",
    "\n",
    "# Normalize\n",
    "query_embedding_nc = F.normalize(query_embedding_nc, p=2, dim=1)\n",
    "\n",
    "# Calculate and print scores\n",
    "scores_nc, top_indices_nc = compute_and_print_scores(query_embedding_nc, passage_embeddings, \"Approach 2\")\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 7. APPROACH 3: Context included in embedding\n",
    "#########################################\n",
    "print(\"\\n\\n### APPROACH 3: Context included in the embedding ###\")\n",
    "\n",
    "# Add the standard EmbeddingGemma query prompt\n",
    "query_prompt = \"task: search result | query: \"\n",
    "\n",
    "# Concatenate context directly into the query (with prompt)\n",
    "query_with_context = f\"{context} {query_prompt}{query}\"\n",
    "query_context_batch = tokenizer([query_with_context], padding=True, return_tensors=\"pt\")\n",
    "query_context_batch = {k: v.to(device) for k, v in query_context_batch.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_context_outputs = base_transformer(**query_context_batch)\n",
    "\n",
    "# Pool over everything (context + text both in embedding)\n",
    "query_embedding_emb = masked_average_pool(\n",
    "    query_context_outputs.last_hidden_state,\n",
    "    query_context_batch[\"attention_mask\"],\n",
    "    label=\"Approach 3 - Query\"\n",
    ")\n",
    "\n",
    "# Apply Dense projection layers\n",
    "query_embedding_emb = apply_dense_layers(query_embedding_emb, dense_layers)\n",
    "\n",
    "# Normalize\n",
    "query_embedding_emb = F.normalize(query_embedding_emb, p=2, dim=1)\n",
    "\n",
    "# Calculate and print scores\n",
    "scores_emb, top_indices_emb = compute_and_print_scores(query_embedding_emb, passage_embeddings, \"Approach 3\")\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 8. APPROACH 4: Standard EmbeddingGemma (for comparison)\n",
    "#########################################\n",
    "print(\"\\n\\n### APPROACH 4: Standard EmbeddingGemma encode_query method ###\")\n",
    "\n",
    "# Use the built-in encode_query method (includes prompting)\n",
    "query_embedding_standard = model.encode_query(query, convert_to_tensor=True)\n",
    "similarities = model.similarity(query_embedding_standard, passage_embeddings) * 100\n",
    "\n",
    "print(f\"\\nApproach 4 - Similarity scores (top 3):\")\n",
    "top_indices_standard = similarities[0].argsort(descending=True)[:3]\n",
    "for idx in top_indices_standard:\n",
    "    print(f\"  Query vs Passage {idx.item() + 1}: {similarities[0, idx].item():.2f}\")\n",
    "\n",
    "# Verification: Check if Approach 2 and 4 match exactly\n",
    "print(f\"\\nVerification: Approach 4 vs Approach 2\")\n",
    "manual_sim = (query_embedding_nc @ passage_embeddings.T) * 100\n",
    "print(f\"  Max difference in scores: {(similarities - manual_sim).abs().max().item():.6f}\")\n",
    "print(f\"  Embeddings identical: {torch.allclose(query_embedding_standard, query_embedding_nc, atol=1e-6)}\")\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 9. Summary\n",
    "#########################################\n",
    "print(\"\\n\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY - EmbeddingGemma with Custom Masked Pooling\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nApproach 1 (Context in attention only - CORE EXPERIMENT):\")\n",
    "print(\"  ✓ Query embedding represents ONLY the query text\")\n",
    "print(\"  ✓ Context influences interpretation via attention\")\n",
    "print(\"  ✓ Query embedding remains comparable to passages\")\n",
    "print(f\"  Top match: Passage {top_indices[0].item() + 1} (score: {scores[0, top_indices[0]].item():.2f})\")\n",
    "\n",
    "print(\"\\nApproach 2 (No context):\")\n",
    "print(\"  ✗ No way to use context to influence query interpretation\")\n",
    "print(f\"  Top match: Passage {top_indices_nc[0].item() + 1} (score: {scores_nc[0, top_indices_nc[0]].item():.2f})\")\n",
    "\n",
    "print(\"\\nApproach 3 (Context in embedding):\")\n",
    "print(\"  ✓ Can use context to influence query\")\n",
    "print(\"  ✗ Query embedding now represents 'context + query', not just query\")\n",
    "print(\"  ✗ Changes the semantic representation of the query itself\")\n",
    "print(f\"  Top match: Passage {top_indices_emb[0].item() + 1} (score: {scores_emb[0, top_indices_emb[0]].item():.2f})\")\n",
    "\n",
    "print(\"\\nApproach 4 (Standard EmbeddingGemma with prompts):\")\n",
    "print(\"  • Uses built-in encode_query with task-specific prompts\")\n",
    "print(\"  • Different from Approach 2 due to prompt engineering\")\n",
    "print(f\"  Top match: Passage {top_indices_standard[0].item() + 1} (score: {similarities[0, top_indices_standard[0]].item():.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY INSIGHT: Approach 1 demonstrates masked pooling where\")\n",
    "print(\"context is used during attention (influencing hidden states)\")\n",
    "print(\"but excluded from the final embedding via selective pooling.\")\n",
    "print(\"This allows context to guide interpretation without changing\")\n",
    "print(\"the semantic space of the query embedding.\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
