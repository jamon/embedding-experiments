{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3bbe208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: My favorite woman in history is Marie Curie\n",
      "Query: query: I think she's most known for being a pharoah, but I'm not sure.  What is she known for?\n",
      "Passages:\n",
      "  1. passage: Marie Antoinette is most known for being the last Queen of France before the French Revolution and her alleged quote 'Let them eat cake'.\n",
      "  2. passage: Cleopatra is most known for being the last pharaoh of Ancient Egypt and her relationships with Julius Caesar and Mark Antony.\n",
      "  3. passage: Joan of Arc is most known for leading the French army against English occupation during the Hundred Years' War.\n",
      "  4. passage: Rosa Parks is most known for refusing to give up her bus seat to a white passenger, sparking the Montgomery Bus Boycott.\n",
      "  5. passage: Marie Curie is most known for being the first woman to win a Nobel Prize and discovering the elements polonium and radium.\n",
      "  6. passage: Frida Kahlo is most known for her self-portraits and paintings that explored identity, postcolonialism, and the female experience.\n",
      "  7. passage: Virginia Woolf is most known for her modernist novels like Mrs. Dalloway and To the Lighthouse.\n",
      "  8. passage: Eleanor Roosevelt is most known for her role in drafting the Universal Declaration of Human Rights and her advocacy for civil rights.\n",
      "  9. passage: Amelia Earhart is most known for being the first female aviator to fly solo across the Atlantic Ocean.\n",
      "\n",
      "\n",
      "### Computing passage embeddings (once) ###\n",
      "\n",
      "\n",
      "### Computing passage embeddings (once) ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Passage embeddings] Pooling Operation:\n",
      "  Input shape: torch.Size([9, 33, 1024])\n",
      "  Pooling mask shape: torch.Size([9, 33])\n",
      "  Batch 0: pooling over 31/33 tokens\n",
      "    (sequence padded from 31 to 33)\n",
      "  Batch 1: pooling over 26/33 tokens\n",
      "    (sequence padded from 26 to 33)\n",
      "  Batch 2: pooling over 25/33 tokens\n",
      "    (sequence padded from 25 to 33)\n",
      "  Output shape: torch.Size([9, 1024])\n",
      "Computed 9 passage embeddings with shape torch.Size([9, 1024])\n",
      "\n",
      "\n",
      "### APPROACH 1: Context influences via attention only ###\n",
      "\n",
      "[Approach 1 - Query] Pooling Operation:\n",
      "  Input shape: torch.Size([1, 41, 1024])\n",
      "  Pooling mask shape: torch.Size([1, 41])\n",
      "  Batch 0: pooling over 29/41 tokens\n",
      "    (sequence padded from 29 to 41)\n",
      "  Output shape: torch.Size([1, 1024])\n",
      "\n",
      "Approach 1 - Similarity scores (top 3):\n",
      "  Query vs Passage 5: 85.40\n",
      "  Query vs Passage 2: 81.55\n",
      "  Query vs Passage 1: 79.33\n",
      "\n",
      "\n",
      "### APPROACH 2: No context (baseline) ###\n",
      "\n",
      "[Approach 2 - Query] Pooling Operation:\n",
      "  Input shape: torch.Size([1, 31, 1024])\n",
      "  Pooling mask shape: torch.Size([1, 31])\n",
      "  Batch 0: pooling over 31/31 tokens\n",
      "  Output shape: torch.Size([1, 1024])\n",
      "\n",
      "Approach 2 - Similarity scores (top 3):\n",
      "  Query vs Passage 2: 83.03\n",
      "  Query vs Passage 6: 80.05\n",
      "  Query vs Passage 9: 79.97\n",
      "\n",
      "\n",
      "### APPROACH 3: Context included in the embedding ###\n",
      "\n",
      "[Approach 3 - Query] Pooling Operation:\n",
      "  Input shape: torch.Size([1, 40, 1024])\n",
      "  Pooling mask shape: torch.Size([1, 40])\n",
      "  Batch 0: pooling over 40/40 tokens\n",
      "  Output shape: torch.Size([1, 1024])\n",
      "\n",
      "Approach 3 - Similarity scores (top 3):\n",
      "  Query vs Passage 5: 86.00\n",
      "  Query vs Passage 2: 81.98\n",
      "  Query vs Passage 1: 79.76\n",
      "\n",
      "\n",
      "### APPROACH 4: Standard E5 embedding calculation ###\n",
      "\n",
      "[Approach 2 - Query] Pooling Operation:\n",
      "  Input shape: torch.Size([1, 31, 1024])\n",
      "  Pooling mask shape: torch.Size([1, 31])\n",
      "  Batch 0: pooling over 31/31 tokens\n",
      "  Output shape: torch.Size([1, 1024])\n",
      "\n",
      "Approach 2 - Similarity scores (top 3):\n",
      "  Query vs Passage 2: 83.03\n",
      "  Query vs Passage 6: 80.05\n",
      "  Query vs Passage 9: 79.97\n",
      "\n",
      "\n",
      "### APPROACH 3: Context included in the embedding ###\n",
      "\n",
      "[Approach 3 - Query] Pooling Operation:\n",
      "  Input shape: torch.Size([1, 40, 1024])\n",
      "  Pooling mask shape: torch.Size([1, 40])\n",
      "  Batch 0: pooling over 40/40 tokens\n",
      "  Output shape: torch.Size([1, 1024])\n",
      "\n",
      "Approach 3 - Similarity scores (top 3):\n",
      "  Query vs Passage 5: 86.00\n",
      "  Query vs Passage 2: 81.98\n",
      "  Query vs Passage 1: 79.76\n",
      "\n",
      "\n",
      "### APPROACH 4: Standard E5 embedding calculation ###\n",
      "\n",
      "Approach 4 - Similarity scores (top 3):\n",
      "  Query vs Passage 2: 83.03\n",
      "  Query vs Passage 6: 80.05\n",
      "  Query vs Passage 9: 79.97\n",
      "\n",
      "Verification: Approach 4 vs Approach 2\n",
      "  Max difference in scores: 0.000000\n",
      "  Embeddings identical: True\n",
      "\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "\n",
      "Approach 1 (Context in attention only):\n",
      "  ✓ Query embedding represents ONLY the query text\n",
      "  ✓ Context influences interpretation via attention\n",
      "  ✓ Query embedding remains comparable to passages\n",
      "  Top match: Passage 5 (score: 85.40)\n",
      "\n",
      "Approach 2 (No context):\n",
      "  ✗ No way to use context to influence query interpretation\n",
      "  Top match: Passage 2 (score: 83.03)\n",
      "\n",
      "Approach 3 (Context in embedding):\n",
      "  ✓ Can use context to influence query\n",
      "  ✗ Query embedding now represents 'context + query', not just query\n",
      "  ✗ Changes the semantic representation of the query itself\n",
      "  Top match: Passage 5 (score: 86.00)\n",
      "\n",
      "Approach 4 (Standard E5):\n",
      "  • Reference implementation using standard average pooling\n",
      "  • Should match Approach 2 exactly\n",
      "  Top match: Passage 2 (score: 83.03)\n",
      "============================================================\n",
      "\n",
      "Approach 4 - Similarity scores (top 3):\n",
      "  Query vs Passage 2: 83.03\n",
      "  Query vs Passage 6: 80.05\n",
      "  Query vs Passage 9: 79.97\n",
      "\n",
      "Verification: Approach 4 vs Approach 2\n",
      "  Max difference in scores: 0.000000\n",
      "  Embeddings identical: True\n",
      "\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "\n",
      "Approach 1 (Context in attention only):\n",
      "  ✓ Query embedding represents ONLY the query text\n",
      "  ✓ Context influences interpretation via attention\n",
      "  ✓ Query embedding remains comparable to passages\n",
      "  Top match: Passage 5 (score: 85.40)\n",
      "\n",
      "Approach 2 (No context):\n",
      "  ✗ No way to use context to influence query interpretation\n",
      "  Top match: Passage 2 (score: 83.03)\n",
      "\n",
      "Approach 3 (Context in embedding):\n",
      "  ✓ Can use context to influence query\n",
      "  ✗ Query embedding now represents 'context + query', not just query\n",
      "  ✗ Changes the semantic representation of the query itself\n",
      "  Top match: Passage 5 (score: 86.00)\n",
      "\n",
      "Approach 4 (Standard E5):\n",
      "  • Reference implementation using standard average pooling\n",
      "  • Should match Approach 2 exactly\n",
      "  Top match: Passage 2 (score: 83.03)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 1. Helper: masked average pooling\n",
    "#########################################\n",
    "\n",
    "def masked_average_pool(last_hidden_states: Tensor,\n",
    "                        pooling_mask: Tensor,\n",
    "                        label: str = \"\") -> Tensor:\n",
    "    \"\"\"\n",
    "    last_hidden_states: [batch, seq_len, hidden_dim]\n",
    "    pooling_mask:       [batch, seq_len] with 1 for tokens to include, 0 otherwise\n",
    "    label:              optional label for logging\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, hidden_dim = last_hidden_states.shape\n",
    "    \n",
    "    if label:\n",
    "        print(f\"\\n[{label}] Pooling Operation:\")\n",
    "        print(f\"  Input shape: {last_hidden_states.shape}\")\n",
    "        print(f\"  Pooling mask shape: {pooling_mask.shape}\")\n",
    "    \n",
    "    # Expand mask to [batch, seq_len, 1]\n",
    "    mask = pooling_mask[..., None].bool()\n",
    "\n",
    "    # Zero-out tokens we don't want to pool over\n",
    "    masked_hidden = last_hidden_states.masked_fill(~mask, 0.0)\n",
    "\n",
    "    # Sum over tokens\n",
    "    summed = masked_hidden.sum(dim=1)\n",
    "\n",
    "    # Count how many tokens are actually included per example\n",
    "    counts = pooling_mask.sum(dim=1)[..., None].clamp(min=1)\n",
    "    \n",
    "    if label:\n",
    "        for i in range(min(batch_size, 3)):  # Show first 3 examples\n",
    "            num_tokens = int(counts[i, 0].item())\n",
    "            total_seq_len = (pooling_mask[i] != 0).sum().item()\n",
    "            print(f\"  Batch {i}: pooling over {num_tokens}/{seq_len} tokens\")\n",
    "            if total_seq_len < seq_len:\n",
    "                print(f\"    (sequence padded from {total_seq_len} to {seq_len})\")\n",
    "\n",
    "    # Return average\n",
    "    result = summed / counts\n",
    "    \n",
    "    if label:\n",
    "        print(f\"  Output shape: {result.shape}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 2. Build inputs with explicit context\n",
    "#########################################\n",
    "\n",
    "def build_inputs_with_context(contexts, texts, tokenizer):\n",
    "    \"\"\"\n",
    "    contexts: list[str]  -- extra context for each example\n",
    "    texts:    list[str]  -- main text we want to embed (\"query: ...\", \"passage: ...\")\n",
    "\n",
    "    Returns:\n",
    "      batch_dict      -- dict with input_ids, attention_mask for the model\n",
    "      pooling_mask    -- tensor [batch, seq_len], 1 only on \"text\" tokens\n",
    "    \"\"\"\n",
    "    cls_id = tokenizer.cls_token_id\n",
    "    sep_id = tokenizer.sep_token_id\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    pooling_masks = []\n",
    "\n",
    "    for ctx, txt in zip(contexts, texts):\n",
    "        # Tokenize WITHOUT special tokens so we can control layout\n",
    "        ctx_ids = tokenizer.encode(ctx, add_special_tokens=False)\n",
    "        txt_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "\n",
    "        # Layout: [CLS] context [SEP] text [SEP]\n",
    "        ids = [cls_id] + ctx_ids + [sep_id] + txt_ids + [sep_id]\n",
    "\n",
    "        # Model attention mask: attend to everything except padding\n",
    "        attn = [1] * len(ids)\n",
    "\n",
    "        # Pooling mask:\n",
    "        #   0 for [CLS], context, and first [SEP]\n",
    "        #   1 for text tokens\n",
    "        #   0 for final [SEP]\n",
    "        num_ctx = len(ctx_ids)\n",
    "        num_txt = len(txt_ids)\n",
    "\n",
    "        pool = (\n",
    "            [0] * (1 + num_ctx + 1) +  # [CLS] + context + [SEP]\n",
    "            [1] * num_txt +            # text tokens\n",
    "            [0]                        # final [SEP]\n",
    "        )\n",
    "\n",
    "        input_ids.append(ids)\n",
    "        attention_masks.append(attn)\n",
    "        pooling_masks.append(pool)\n",
    "\n",
    "    # Pad sequences to same length for batching\n",
    "    batch = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids, \"attention_mask\": attention_masks},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # We can reuse tokenizer.pad just to pad the pooling masks with 0\n",
    "    pooling_masks = tokenizer.pad(\n",
    "        {\"input_ids\": pooling_masks},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    return batch, pooling_masks\n",
    "\n",
    "\n",
    "def compute_and_print_scores(query_embedding: Tensor, \n",
    "                              passage_embeddings: Tensor,\n",
    "                              approach_name: str) -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute similarity scores and print top 3 results.\n",
    "    Returns the scores tensor and top indices.\n",
    "    \"\"\"\n",
    "    scores = (query_embedding @ passage_embeddings.T) * 100\n",
    "    print(f\"\\n{approach_name} - Similarity scores (top 3):\")\n",
    "    top_indices = scores[0].argsort(descending=True)[:3]\n",
    "    for idx in top_indices:\n",
    "        print(f\"  Query vs Passage {idx.item() + 1}: {scores[0, idx].item():.2f}\")\n",
    "    return scores, top_indices\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 3. Example data - CONTRIVED SCENARIO\n",
    "#########################################\n",
    "\n",
    "query = \"query: I think she's most known for being a pharoah, but I'm not sure.  What is she known for?\"\n",
    "\n",
    "passages = [\n",
    "    \"passage: Marie Antoinette is most known for being the last Queen of France before the French Revolution and her alleged quote 'Let them eat cake'.\",\n",
    "    \"passage: Cleopatra is most known for being the last pharaoh of Ancient Egypt and her relationships with Julius Caesar and Mark Antony.\",\n",
    "    \"passage: Joan of Arc is most known for leading the French army against English occupation during the Hundred Years' War.\",\n",
    "    \"passage: Rosa Parks is most known for refusing to give up her bus seat to a white passenger, sparking the Montgomery Bus Boycott.\",\n",
    "    \"passage: Marie Curie is most known for being the first woman to win a Nobel Prize and discovering the elements polonium and radium.\",\n",
    "    \"passage: Frida Kahlo is most known for her self-portraits and paintings that explored identity, postcolonialism, and the female experience.\",\n",
    "    \"passage: Virginia Woolf is most known for her modernist novels like Mrs. Dalloway and To the Lighthouse.\",\n",
    "    \"passage: Eleanor Roosevelt is most known for her role in drafting the Universal Declaration of Human Rights and her advocacy for civil rights.\",\n",
    "    \"passage: Amelia Earhart is most known for being the first female aviator to fly solo across the Atlantic Ocean.\",\n",
    "]\n",
    "\n",
    "context = \"My favorite woman in history is Marie Curie\"\n",
    "\n",
    "# Print the data we'll be working with\n",
    "print(\"Context:\", context)\n",
    "print(\"Query:\", query)\n",
    "print(\"Passages:\")\n",
    "for i, passage in enumerate(passages):\n",
    "    print(f\"  {i+1}. {passage}\")\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 4. Load model & tokenizer\n",
    "#########################################\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/e5-large-v2\")\n",
    "model = AutoModel.from_pretrained(\"intfloat/e5-large-v2\")\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 4.5. Compute passage embeddings ONCE (used by all approaches)\n",
    "#########################################\n",
    "print(\"\\n\\n### Computing passage embeddings (once) ###\")\n",
    "\n",
    "passage_batch = tokenizer(passages, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    passage_outputs = model(**passage_batch)\n",
    "\n",
    "# Standard average pooling for passages\n",
    "passage_embeddings = masked_average_pool(\n",
    "    passage_outputs.last_hidden_state,\n",
    "    passage_batch[\"attention_mask\"],\n",
    "    label=\"Passage embeddings\"\n",
    ")\n",
    "passage_embeddings = F.normalize(passage_embeddings, p=2, dim=1)\n",
    "print(f\"Computed {len(passages)} passage embeddings with shape {passage_embeddings.shape}\")\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 5. APPROACH 1: Context in attention, NOT in pooling\n",
    "#########################################\n",
    "print(\"\\n\\n### APPROACH 1: Context influences via attention only ###\")\n",
    "\n",
    "# Only compute query embedding with context\n",
    "batch, pooling_mask = build_inputs_with_context(\n",
    "    contexts=[context],\n",
    "    texts=[query],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],   # <- attention over context + text\n",
    "    )\n",
    "\n",
    "last_hidden = outputs.last_hidden_state\n",
    "\n",
    "# Pool ONLY over the \"main text\" tokens (exclude context)\n",
    "query_embedding = masked_average_pool(last_hidden, pooling_mask, label=\"Approach 1 - Query\")\n",
    "query_embedding = F.normalize(query_embedding, p=2, dim=1)\n",
    "\n",
    "# Calculate and print scores\n",
    "scores, top_indices = compute_and_print_scores(query_embedding, passage_embeddings, \"Approach 1\")\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 6. APPROACH 2: NO context at all\n",
    "#########################################\n",
    "print(\"\\n\\n### APPROACH 2: No context (baseline) ###\")\n",
    "\n",
    "# Just embed the query without any context\n",
    "no_context_batch = tokenizer([query], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    no_context_outputs = model(**no_context_batch)\n",
    "\n",
    "# Standard average pooling over all non-padding tokens\n",
    "query_embedding_nc = masked_average_pool(\n",
    "    no_context_outputs.last_hidden_state,\n",
    "    no_context_batch[\"attention_mask\"],\n",
    "    label=\"Approach 2 - Query\"\n",
    ")\n",
    "query_embedding_nc = F.normalize(query_embedding_nc, p=2, dim=1)\n",
    "\n",
    "# Calculate and print scores\n",
    "scores_nc, top_indices_nc = compute_and_print_scores(query_embedding_nc, passage_embeddings, \"Approach 2\")\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 7. APPROACH 3: Context included in embedding\n",
    "#########################################\n",
    "print(\"\\n\\n### APPROACH 3: Context included in the embedding ###\")\n",
    "\n",
    "# Concatenate context directly into the query\n",
    "query_with_context = f\"{context} {query}\"\n",
    "embedded_batch = tokenizer([query_with_context], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedded_outputs = model(**embedded_batch)\n",
    "\n",
    "# Pool over everything (context + text both in embedding for query)\n",
    "query_embedding_emb = masked_average_pool(\n",
    "    embedded_outputs.last_hidden_state,\n",
    "    embedded_batch[\"attention_mask\"],\n",
    "    label=\"Approach 3 - Query\"\n",
    ")\n",
    "query_embedding_emb = F.normalize(query_embedding_emb, p=2, dim=1)\n",
    "\n",
    "# Calculate and print scores\n",
    "scores_emb, top_indices_emb = compute_and_print_scores(query_embedding_emb, passage_embeddings, \"Approach 3\")\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 8. APPROACH 4: Standard E5 embedding (reference implementation)\n",
    "#########################################\n",
    "print(\"\\n\\n### APPROACH 4: Standard E5 embedding calculation ###\")\n",
    "\n",
    "# Use the standard E5 approach: average pool over all tokens (excluding padding)\n",
    "# This should match Approach 2 exactly\n",
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    \"\"\"Standard average pooling used by E5 and similar models.\"\"\"\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "# Embed query without context\n",
    "batch_standard = tokenizer([query], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_standard = model(**batch_standard)\n",
    "\n",
    "query_embedding_standard = average_pool(outputs_standard.last_hidden_state, batch_standard[\"attention_mask\"])\n",
    "query_embedding_standard = F.normalize(query_embedding_standard, p=2, dim=1)\n",
    "\n",
    "# Calculate and print scores\n",
    "scores_standard, top_indices_standard = compute_and_print_scores(query_embedding_standard, passage_embeddings, \"Approach 4\")\n",
    "\n",
    "# Verify this matches Approach 2\n",
    "print(f\"\\nVerification: Approach 4 vs Approach 2\")\n",
    "print(f\"  Max difference in scores: {(scores_standard - scores_nc).abs().max().item():.6f}\")\n",
    "print(f\"  Embeddings identical: {torch.allclose(query_embedding_standard, query_embedding_nc, atol=1e-6)}\")\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 9. Summary\n",
    "#########################################\n",
    "print(\"\\n\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nApproach 1 (Context in attention only):\")\n",
    "print(\"  ✓ Query embedding represents ONLY the query text\")\n",
    "print(\"  ✓ Context influences interpretation via attention\")\n",
    "print(\"  ✓ Query embedding remains comparable to passages\")\n",
    "print(f\"  Top match: Passage {top_indices[0].item() + 1} (score: {scores[0, top_indices[0]].item():.2f})\")\n",
    "\n",
    "print(\"\\nApproach 2 (No context):\")\n",
    "print(\"  ✗ No way to use context to influence query interpretation\")\n",
    "print(f\"  Top match: Passage {top_indices_nc[0].item() + 1} (score: {scores_nc[0, top_indices_nc[0]].item():.2f})\")\n",
    "\n",
    "print(\"\\nApproach 3 (Context in embedding):\")\n",
    "print(\"  ✓ Can use context to influence query\")\n",
    "print(\"  ✗ Query embedding now represents 'context + query', not just query\")\n",
    "print(\"  ✗ Changes the semantic representation of the query itself\")\n",
    "print(f\"  Top match: Passage {top_indices_emb[0].item() + 1} (score: {scores_emb[0, top_indices_emb[0]].item():.2f})\")\n",
    "\n",
    "print(\"\\nApproach 4 (Standard E5):\")\n",
    "print(\"  • Reference implementation using standard average pooling\")\n",
    "print(\"  • Should match Approach 2 exactly\")\n",
    "print(f\"  Top match: Passage {top_indices_standard[0].item() + 1} (score: {scores_standard[0, top_indices_standard[0]].item():.2f})\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
